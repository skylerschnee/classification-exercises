{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bdc59c5",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "[Course content](https://ds.codeup.com/classification/evaluation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae746ca",
   "metadata": {},
   "source": [
    "**Objective:** Understand and apply various metrics used to evaluate the performance of a classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b01baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5b4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### A dataframe which contains predicted values and actual values\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'actual': ['coffee', 'no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'coffee'],\n",
    "    'prediction': ['no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'no coffee'],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d46e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual prediction\n",
       "0     coffee  no coffee\n",
       "1  no coffee  no coffee\n",
       "2  no coffee     coffee\n",
       "3     coffee     coffee\n",
       "4     coffee     coffee\n",
       "5     coffee     coffee\n",
       "6  no coffee  no coffee\n",
       "7     coffee  no coffee"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### View the dataframe\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9a6d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prediction</th>\n",
       "      <th>coffee</th>\n",
       "      <th>no coffee</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no coffee</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prediction  coffee  no coffee\n",
       "actual                       \n",
       "coffee           3          2\n",
       "no coffee        1          2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Use a crosstab to count the outcomes\n",
    "\n",
    "pd.crosstab(df.actual, df.prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc331d8",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "The two outcomes in classification are labeled as either **positive** or **negative**. \n",
    "\n",
    "\n",
    "While the designations are arbitrary, they impact how evaluation metrics are interpreted. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c5c27",
   "metadata": {},
   "source": [
    "### Evaluation on train, test, and split\n",
    "\n",
    "\n",
    "| Split |  Purpose |\n",
    "| ----------- | :----------- |\n",
    "| Train | Evaluate in-sample performance|\n",
    "| Validate |  Evaluate out of sample performance to tune hyper-parameters |\n",
    "| Test | Evaluate performance of model |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a265e7",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A diagram which summarizes the outcomes of a model. \n",
    "\n",
    "\n",
    "\n",
    "| Designation      | Description |\n",
    "| ----------- | ----------- |\n",
    "| True Negative      | Model correctly predicted the negative outcome       |\n",
    "| False Positive   | Model incorrectly predicted the positive outcome        |\n",
    "| False Negative   | Model incorrectly predicted the negative outcome        |\n",
    "| True Positive      | Model correctly predicted the positive outcome       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53ba95c",
   "metadata": {},
   "source": [
    "### Confusion Matrix with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e58a9",
   "metadata": {},
   "source": [
    "'coffee' is the positive outcome`\n",
    "\n",
    "'no coffee' is the negative outcome\n",
    "\n",
    "\n",
    "The function `confusion_matrix` returns a 2x2 array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64bc14",
   "metadata": {},
   "source": [
    "### Components of a confusion matrix\n",
    " \n",
    " For a confusion matrix $C$,\n",
    "\n",
    "\n",
    "| Index (row, column)      | Count of |\n",
    "| ----------- | ----------- |\n",
    "| $C_{0,0}$      | True Negatives       |\n",
    "| $C_{1,0}$    |   False Negatives      |\n",
    "| $C_{1,1}$    |   True Positives      |\n",
    "| $C_{0,1}$    |   False Positives      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ab7dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Return a confusion matrix for the model's predictions\n",
    "\n",
    "\n",
    "confusion_matrix(df.actual, df.prediction, labels = ('no coffee','coffee'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671d000",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba22ffe3",
   "metadata": {},
   "source": [
    "### Accuracy \n",
    "\n",
    "Accuracy evaluates how many correct predictions (both positive and negative) were made over the total number of predictions. \n",
    "\n",
    "\n",
    "$\\texttt{Accuracy} = \\dfrac{TP + TN}{TP + FP + FN + TN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447dda2a",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision evaluates how many of the positive predictions were correct.\n",
    "\n",
    "$\\texttt{Precision} = \\dfrac{TP}{TP + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ec1a6",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall evaluates how the model handled all positive outcomes. \n",
    "\n",
    "$\\texttt{Recall} = \\dfrac{TP}{TP + FN}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0cd4e9",
   "metadata": {},
   "source": [
    "### Misclassification Rate\n",
    "\n",
    "Misclassification rate concerns how many predictions were incorrect. This accounts for all other outcomes not included in the calculation of accuracy. \n",
    "\n",
    "$\\texttt{Misclassification Rate} = 1 - \\texttt{Accuracy}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c2ff35",
   "metadata": {},
   "source": [
    "### Sensitivity (True Positive Rate)\n",
    "\n",
    "\n",
    "$\\texttt{True Positive Rate} = \\dfrac{TP}{TP + FN} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ab289",
   "metadata": {},
   "source": [
    "### Specificity \n",
    "\n",
    "How well does the model predict negative outcomes?\n",
    "\n",
    "\n",
    "$\\texttt{Specificity} = \\dfrac{TN}{FP + TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab1564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19cde0ca",
   "metadata": {},
   "source": [
    "### Negative Predictive Value\n",
    "\n",
    "$\\texttt{NPV} = \\dfrac{TN}{TN + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48169b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1287099b",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "$\\texttt{F1  Score} = 2 * \\dfrac{Precision * Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c00ff",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "The baseline is a simple model that is a reference point for the performance of other models. \n",
    "\n",
    "For a classification model, a baseline is often the mode.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b07798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coffee       5\n",
       "no coffee    3\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Find the counts of each outcome\n",
    "df.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c631406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the baseline_prediction to be coffee\n",
    "df['baseline_prediction'] = 'coffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fa213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077b1d3d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### Evaluation Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30257090",
   "metadata": {},
   "source": [
    "## Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3c9a639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comapre the models prediction to actual\n",
    "model_accuracy = (df.prediction == df.actual).mean()\n",
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bf777c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare the baseline to actual\n",
    "baseline_accuarcy = (df.baseline_prediction == df.actual).mean()\n",
    "baseline_accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc780c0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: invalid syntax (987084884.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [32]\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f'Model Accuracy: {model_accuracy.2f}')\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(f'Model Accuracy: {model_accuracy.2f}')\n",
    "print(f'Model Accuracy: {baseline_accuracy.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e924b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366dc6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3253203",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e2e45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53024f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#restrict to positive values ('coffee') for the actual values\n",
    "\n",
    "subset = df[df.actual == 'coffee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0989cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Recall\n",
    "\n",
    "model_recall = (subset.prediction == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21f93d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Recall\n",
    "\n",
    "baseline_recall = model_recall = (subset.baseline_prediction == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc32f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model recall: 100.00%\n",
      "Baseline recall: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# SOMETHING IS WRONG HERE\n",
    "\n",
    "print(f'Model recall: {model_recall:.2%}')\n",
    "print(f'Baseline recall: {baseline_recall:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3364c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5419b8ba",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6d9aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restrict to positive values ('coffee') for the PREDICTED values\n",
    "subset = df[df.prediction == 'coffee']\n",
    "\n",
    "#Model Precision\n",
    "model_precision = (subset.prediction == subset.actual).mean()\n",
    "\n",
    "#Baseline Precision\n",
    "subset = df[df.baseline_prediciton == 'coffee']\n",
    "baseline_precision = (subset.baseline_prediciton == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42e7efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model precision: 75.00%\n",
      "Baseline precision: 62.50%\n"
     ]
    }
   ],
   "source": [
    "print(f'Model precision: {model_precision:.2%}')\n",
    "print(f'Baseline precision: {baseline_precision:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3e8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
